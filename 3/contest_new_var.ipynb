{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import xgboost as xgb\n",
    "import pandas as pd\n",
    "#from xgboost.sklearn import XGBRegressor\n",
    "#from sklearn import model_selection, metrics\n",
    "import matplotlib.pyplot as pl\n",
    "import itertools\n",
    "import scipy.optimize as so\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.tsv\")\n",
    "train.drop([\"f{}\".format(i) for i in range(31, 61)], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count = np.zeros([4, 53, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(train[par1])):\n",
    "    count[train['year'].values[i] - 2012][train['week'].values[i] - 1][train['shift'].values[i] - 1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unused_indexes = [(i, j, k) for i, year in enumerate(count)\n",
    "                  for j, week in enumerate(year)\n",
    "                  for k, shift in enumerate(week)\n",
    "                  if shift == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72457"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[main_features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "koefs = np.zeros([4, 53, 3])\n",
    "indexes = [[[[] for k in range(3)] for j in range(53)] for i in range(4)]\n",
    "for i in range(len(train['year'])):\n",
    "    indexes[train['year'].values[i] - 2012][train['week'].values[i] - 1][train['shift'].values[i] - 1].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for year in range(4):\n",
    "    for week in range(53):\n",
    "        for shift in range(3):\n",
    "            x_val = [i.sum() for i in train[main_features].values[indexes[year][week][shift]]]\n",
    "            y_val = train['y'].values[indexes[year][week][shift]]\n",
    "            koefs[year, week, shift] = so.minimize(lambda x: sum(np.abs((x*x_val - y_val))), 2).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 51 0\n",
      "1 0 0\n",
      "1 0 1\n",
      "1 1 0\n",
      "1 1 1\n",
      "1 1 2\n",
      "1 2 0\n",
      "1 2 1\n",
      "1 2 2\n",
      "1 3 0\n",
      "1 3 1\n",
      "1 3 2\n",
      "1 4 0\n",
      "1 4 1\n",
      "1 4 2\n",
      "1 5 0\n",
      "1 5 1\n",
      "1 5 2\n",
      "1 6 0\n",
      "1 6 1\n",
      "1 6 2\n",
      "1 7 0\n",
      "1 7 1\n",
      "1 7 2\n",
      "1 8 0\n",
      "1 8 1\n",
      "1 8 2\n",
      "1 9 0\n",
      "1 9 1\n",
      "1 9 2\n",
      "1 10 0\n",
      "1 10 1\n",
      "1 10 2\n",
      "1 11 0\n",
      "1 11 1\n",
      "1 11 2\n",
      "1 12 0\n",
      "1 12 1\n",
      "1 12 2\n",
      "1 13 0\n",
      "1 13 1\n",
      "1 13 2\n",
      "1 14 0\n",
      "1 14 1\n",
      "1 14 2\n",
      "1 15 0\n",
      "1 15 1\n",
      "1 15 2\n",
      "1 16 0\n",
      "1 16 1\n",
      "1 16 2\n",
      "1 17 0\n",
      "1 17 1\n",
      "1 17 2\n",
      "1 18 0\n",
      "1 18 1\n",
      "1 18 2\n",
      "1 19 0\n",
      "1 19 1\n",
      "1 19 2\n",
      "1 20 0\n",
      "1 20 1\n",
      "1 20 2\n",
      "1 21 0\n",
      "1 21 1\n",
      "1 21 2\n",
      "1 22 0\n",
      "1 22 1\n",
      "1 22 2\n",
      "1 23 0\n",
      "1 23 1\n",
      "1 23 2\n",
      "1 24 0\n",
      "1 24 1\n",
      "1 24 2\n",
      "1 25 0\n",
      "1 25 1\n",
      "1 25 2\n",
      "1 26 0\n",
      "1 26 1\n",
      "1 26 2\n",
      "1 27 0\n",
      "1 27 1\n",
      "1 27 2\n",
      "1 28 0\n",
      "1 28 1\n",
      "1 28 2\n",
      "1 29 0\n",
      "1 29 1\n",
      "1 29 2\n",
      "1 30 0\n",
      "1 30 1\n",
      "1 30 2\n",
      "1 31 0\n",
      "1 31 1\n",
      "1 31 2\n",
      "1 32 0\n",
      "1 32 1\n",
      "1 32 2\n",
      "1 33 0\n",
      "1 33 1\n",
      "1 33 2\n",
      "1 34 0\n",
      "1 34 1\n",
      "1 34 2\n",
      "1 35 0\n",
      "1 35 1\n",
      "1 35 2\n",
      "1 36 0\n",
      "1 36 1\n",
      "1 36 2\n",
      "1 37 0\n",
      "1 37 1\n",
      "1 37 2\n",
      "1 38 0\n",
      "1 38 1\n",
      "1 38 2\n",
      "1 39 0\n",
      "1 39 1\n",
      "1 39 2\n",
      "1 40 0\n",
      "1 40 1\n",
      "1 40 2\n",
      "1 41 0\n",
      "1 41 1\n",
      "1 41 2\n",
      "1 42 0\n",
      "1 42 1\n",
      "1 42 2\n",
      "1 43 0\n",
      "1 43 1\n",
      "1 43 2\n",
      "1 44 0\n",
      "1 44 1\n",
      "1 44 2\n",
      "1 45 0\n",
      "1 45 1\n",
      "1 45 2\n",
      "1 46 0\n",
      "1 46 1\n",
      "1 46 2\n",
      "1 47 0\n",
      "1 47 1\n",
      "1 47 2\n",
      "1 48 0\n",
      "1 48 1\n",
      "1 48 2\n",
      "1 49 0\n",
      "1 49 1\n",
      "1 49 2\n",
      "1 50 0\n",
      "1 50 1\n",
      "1 50 2\n",
      "1 51 0\n",
      "1 51 1\n",
      "1 51 2\n",
      "2 0 0\n",
      "2 0 1\n",
      "2 0 2\n",
      "2 1 0\n",
      "2 1 1\n",
      "2 1 2\n",
      "2 2 0\n",
      "2 2 1\n",
      "2 2 2\n",
      "2 3 0\n",
      "2 3 1\n",
      "2 3 2\n",
      "2 4 0\n",
      "2 4 1\n",
      "2 4 2\n",
      "2 5 0\n",
      "2 5 1\n",
      "2 5 2\n",
      "2 6 0\n",
      "2 6 1\n",
      "2 6 2\n",
      "2 7 0\n",
      "2 7 1\n",
      "2 7 2\n",
      "2 8 0\n",
      "2 8 1\n",
      "2 8 2\n",
      "2 9 0\n",
      "2 9 1\n",
      "2 9 2\n",
      "2 10 0\n",
      "2 10 1\n",
      "2 10 2\n",
      "2 11 0\n",
      "2 11 1\n",
      "2 11 2\n",
      "2 12 0\n",
      "2 12 1\n",
      "2 12 2\n",
      "2 13 0\n",
      "2 13 1\n",
      "2 13 2\n",
      "2 14 0\n",
      "2 14 1\n",
      "2 14 2\n",
      "2 15 0\n",
      "2 15 1\n",
      "2 15 2\n",
      "2 16 0\n",
      "2 16 1\n",
      "2 16 2\n",
      "2 17 0\n",
      "2 17 1\n",
      "2 17 2\n",
      "2 18 0\n",
      "2 18 1\n",
      "2 18 2\n",
      "2 19 0\n",
      "2 19 1\n",
      "2 19 2\n",
      "2 20 0\n",
      "2 20 1\n",
      "2 20 2\n",
      "2 21 0\n",
      "2 21 1\n",
      "2 21 2\n",
      "2 22 0\n",
      "2 22 1\n",
      "2 22 2\n",
      "2 23 0\n",
      "2 23 1\n",
      "2 23 2\n",
      "2 24 0\n",
      "2 24 1\n",
      "2 24 2\n",
      "2 25 0\n",
      "2 25 1\n",
      "2 25 2\n",
      "2 26 0\n",
      "2 26 1\n",
      "2 26 2\n",
      "2 27 0\n",
      "2 27 1\n",
      "2 27 2\n",
      "2 28 0\n",
      "2 28 1\n",
      "2 28 2\n",
      "2 29 0\n",
      "2 29 1\n",
      "2 29 2\n",
      "2 30 0\n",
      "2 30 1\n",
      "2 30 2\n",
      "2 31 0\n",
      "2 31 1\n",
      "2 31 2\n",
      "2 32 0\n",
      "2 32 1\n",
      "2 32 2\n",
      "2 33 0\n",
      "2 33 1\n",
      "2 33 2\n",
      "2 34 0\n",
      "2 34 1\n",
      "2 34 2\n",
      "2 35 0\n",
      "2 35 1\n",
      "2 35 2\n",
      "2 36 0\n",
      "2 36 1\n",
      "2 36 2\n",
      "2 37 0\n",
      "2 37 1\n",
      "2 37 2\n",
      "2 38 0\n",
      "2 38 1\n",
      "2 38 2\n",
      "2 39 0\n",
      "2 39 1\n",
      "2 39 2\n",
      "2 40 0\n",
      "2 40 1\n",
      "2 40 2\n",
      "2 41 0\n",
      "2 41 1\n",
      "2 41 2\n",
      "2 42 0\n",
      "2 42 1\n",
      "2 42 2\n",
      "2 43 0\n",
      "2 43 1\n",
      "2 43 2\n",
      "2 44 0\n",
      "2 44 1\n",
      "2 44 2\n",
      "2 45 0\n",
      "2 45 1\n",
      "2 45 2\n",
      "2 46 0\n",
      "2 46 1\n",
      "2 46 2\n",
      "2 47 0\n",
      "2 47 1\n",
      "2 47 2\n",
      "2 48 0\n",
      "2 48 1\n",
      "2 48 2\n",
      "2 49 0\n",
      "2 49 1\n",
      "2 49 2\n",
      "2 50 0\n",
      "2 50 1\n",
      "2 50 2\n",
      "2 51 0\n",
      "2 51 1\n",
      "2 51 2\n",
      "2 52 0\n",
      "2 52 1\n",
      "2 52 2\n",
      "3 0 0\n",
      "3 0 1\n",
      "3 0 2\n",
      "3 1 0\n",
      "3 1 1\n",
      "3 1 2\n"
     ]
    }
   ],
   "source": [
    "koefs = np.zeros([4, 53, 3, 30])\n",
    "main_features = [\"f{}\".format(i) for i in range(1, 31)]\n",
    "for i in range(4):\n",
    "    for j in range(53):\n",
    "        for k in range(3):\n",
    "            if (i, j, k) not in unused_indexes:\n",
    "                print i, j, k\n",
    "                answer = np.array([train['y'].values[ind] for ind in range(len(train['y']))\n",
    "                                   if train['year'].values[ind] == i + 2012 and\n",
    "                                   train['week'].values[ind] == j + 1and\n",
    "                                   train['shift'].values[ind] == k + 1])\n",
    "                A = np.array([train[main_features].values[ind] for ind in range(len(train['y']))\n",
    "                              if train['year'].values[ind] == i + 2012 and\n",
    "                              train['week'].values[ind] == j + 1 and\n",
    "                              train['shift'].values[ind] == k + 1])\n",
    "                koefs[i][j][k] = np.dot(np.linalg.inv(np.dot(A.T, A)), np.dot(A.T, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5695.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4695.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>33560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>82560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72427</th>\n",
       "      <td>8730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72428</th>\n",
       "      <td>232279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72429</th>\n",
       "      <td>16334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72430</th>\n",
       "      <td>2050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72431</th>\n",
       "      <td>525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72432</th>\n",
       "      <td>25121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72433</th>\n",
       "      <td>620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72434</th>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72435</th>\n",
       "      <td>104728.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72436</th>\n",
       "      <td>645081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72437</th>\n",
       "      <td>283809.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72438</th>\n",
       "      <td>611395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72439</th>\n",
       "      <td>24735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72440</th>\n",
       "      <td>6630.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72441</th>\n",
       "      <td>28910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72442</th>\n",
       "      <td>468998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72443</th>\n",
       "      <td>108390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72444</th>\n",
       "      <td>202381.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72445</th>\n",
       "      <td>521192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72446</th>\n",
       "      <td>755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72447</th>\n",
       "      <td>570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72448</th>\n",
       "      <td>690.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72449</th>\n",
       "      <td>571.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72450</th>\n",
       "      <td>8911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72451</th>\n",
       "      <td>60839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72452</th>\n",
       "      <td>25348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72453</th>\n",
       "      <td>23319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72454</th>\n",
       "      <td>515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72455</th>\n",
       "      <td>5597.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72456</th>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72457 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1\n",
       "0        4915.0\n",
       "1        2230.0\n",
       "2        5695.0\n",
       "3        1995.0\n",
       "4        6515.0\n",
       "5         445.0\n",
       "6         445.0\n",
       "7        1167.0\n",
       "8        1640.0\n",
       "9        1440.0\n",
       "10       4695.0\n",
       "11          0.0\n",
       "12          5.0\n",
       "13         10.0\n",
       "14         80.0\n",
       "15         10.0\n",
       "16      33560.0\n",
       "17      82560.0\n",
       "18      13640.0\n",
       "19        228.0\n",
       "20        160.0\n",
       "21       3845.0\n",
       "22          0.0\n",
       "23          0.0\n",
       "24        865.0\n",
       "25       2310.0\n",
       "26          0.0\n",
       "27       1730.0\n",
       "28        235.0\n",
       "29        920.0\n",
       "...         ...\n",
       "72427    8730.0\n",
       "72428  232279.0\n",
       "72429   16334.0\n",
       "72430    2050.0\n",
       "72431     525.0\n",
       "72432   25121.0\n",
       "72433     620.0\n",
       "72434     120.0\n",
       "72435  104728.0\n",
       "72436  645081.0\n",
       "72437  283809.0\n",
       "72438  611395.0\n",
       "72439   24735.0\n",
       "72440    6630.0\n",
       "72441   28910.0\n",
       "72442  468998.0\n",
       "72443  108390.0\n",
       "72444  202381.0\n",
       "72445  521192.0\n",
       "72446     755.0\n",
       "72447     570.0\n",
       "72448     690.0\n",
       "72449     571.0\n",
       "72450    8911.0\n",
       "72451   60839.0\n",
       "72452   25348.0\n",
       "72453   23319.0\n",
       "72454     515.0\n",
       "72455    5597.0\n",
       "72456      10.0\n",
       "\n",
       "[72457 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[[\"f{}\".format(i) for i in range(1, 2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ff431637590>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEDCAYAAADX1GjKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFehJREFUeJzt3X+QHOV95/H3V8sCS3C8YG1SsBKWfKfIUYJjkT2Mj9Qd\nsR1LcBdQESeRYpd9CRfqfpDKD0cpqezCNpc6cFRxYldIbCWhfOc4/LBDFNkmJ/sCqVylAmF1CwiB\n11YIWFp80QYQ/oNNkOTv/TE98mg1szs7mpnemXm/qrbU80zP9LdVs/PZfp5+uiMzkSQNthVlFyBJ\nKp9hIEkyDCRJhoEkCcNAkoRhIEmi5DCIiLsi4mhEPNnEupdFxEMRMRURT0TEdd2oUZIGQdlHBp8G\nNje57geB+zJzI7AV+N1OFSVJg6bUMMjMvwJerG2LiH8REf8rIvZHxP+JiDdWVwe+u1h+LfB8F0uV\npL52TtkF1LEb+E+Z+fWIeAuVI4C3AR8GvhwRvwB8F/CO8kqUpP6yrMIgIi4E/jXwuYioNp9X/LsN\n+HRm/mZEvBX4TET8YGZ+u4RSJamvLKswoNJtdSwz31znuZsoxhcy828i4nxgJXC0i/VJUl8qewD5\nNJn5LeDvI+InAaLih4qnvwG8vWj/fuB8YLaUQiWpz0SZVy2NiLuBa6j8hf8PwIeAB4HfAy4BhoF7\nMvO2iNgA/D5wIZXB5F/LzC+XUbck9ZtSw0CStDwsq24iSVI5ShtAXrlyZa5Zs6aszUtST9q/f/8/\nZuZYu9+3tDBYs2YNk5OTZW1eknpSRDzXife1m0iSZBhIkgwDSRKGgSQJw0CSxPK7NpEk9a09UzPs\n2jfN88fmuHR0hO2b1rNl43jZZQGGgSR1xZ6pGXbef4C54ycBmDk2x877DwAsi0Cwm0iSumDXvulT\nQVA1d/wku/ZNl1TR6QwDSeqC54/NLam92wwDSeqCS0dHltTebYaBJHXB9k3rGRkeOq1tZHiI7ZvW\nl1TR6RxAlqQuqA4SezaRJA24LRvHl82X/3x2E0mSDANJkmEgScIwkCRhGEiSMAwkSRgGkiQMA0kS\nhoEkCcNAkkQTYRARd0XE0Yh4ssHzERGfiIhDEfFERFzR/jIlSZ3UzJHBp4HNCzx/LbCu+LkZ+L2z\nL0uS1E2LhkFm/hXw4gKr3AD8z6x4GBiNiEvaVaAkqfPaMWYwDhyueXykaJMk9YiuDiBHxM0RMRkR\nk7Ozs93ctCRpAe0Igxlgdc3jVUXbGTJzd2ZOZObE2NhYGzYtSWqHdoTBXuC9xVlFVwEvZ+Y32/C+\nkqQuWfROZxFxN3ANsDIijgAfAoYBMvOTwAPAdcAh4BXgZztVrCSpMxYNg8zctsjzCfzXtlUkSeo6\nZyBLkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaS\nJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEmiyTCIiM0RMR0R\nhyJiR53nL4uIhyJiKiKeiIjr2l+qJKlTFg2DiBgC7gSuBTYA2yJiw7zVPgjcl5kbga3A77a7UElS\n5zRzZHAlcCgzn8nMV4F7gBvmrZPAdxfLrwWeb1+JkqROayYMxoHDNY+PFG21Pgy8JyKOAA8Av1Dv\njSLi5oiYjIjJ2dnZFsqVJHVCuwaQtwGfzsxVwHXAZyLijPfOzN2ZOZGZE2NjY23atCTpbDUTBjPA\n6prHq4q2WjcB9wFk5t8A5wMr21GgJKnzmgmDR4F1EbE2Is6lMkC8d9463wDeDhAR308lDOwHkqQe\nsWgYZOYJ4BZgH/A0lbOGDkbEbRFxfbHa+4Gfj4jHgbuB/5CZ2amiJUntdU4zK2XmA1QGhmvbbq1Z\nfgq4ur2lSZK6xRnIkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkmjy2kSS\nOmvP1Ay79k3z/LE5Lh0dYfum9WzZOP8eUlLnGAZSyfZMzbDz/gPMHT8JwMyxOXbefwDAQFDX2E0k\nlWzXvulTQVA1d/wku/ZNl1SRBpFhIJXs+WNzS2qXOsEwkEp26ejIktqlTjAMpJJt37SekeGh09pG\nhofYvml9SRVpEDmALJWsOkjs2UQqk2EgLQNbNo775a9S2U0kSTIMJEmGgSQJxwykBXmZCA0Kw0Bq\nwMtEaJDYTSQ14GUiNEgMA6kBLxOhQWIYSA14mQgNkqbCICI2R8R0RByKiB0N1vmpiHgqIg5GxB+3\nt0yp+7xMhAbJogPIETEE3An8GHAEeDQi9mbmUzXrrAN2Aldn5ksR8T2dKljqFi8ToUHSzNlEVwKH\nMvMZgIi4B7gBeKpmnZ8H7szMlwAy82i7C5XK4GUiNCia6SYaBw7XPD5StNX6PuD7IuKvI+LhiNhc\n740i4uaImIyIydnZ2dYqliS1XbsGkM8B1gHXANuA34+I0fkrZebuzJzIzImxsbE2bVqSdLaaCYMZ\nYHXN41VFW60jwN7MPJ6Zfw98jUo4SJJ6QDNh8CiwLiLWRsS5wFZg77x19lA5KiAiVlLpNnqmjXVK\nkjpo0TDIzBPALcA+4Gngvsw8GBG3RcT1xWr7gBci4ingIWB7Zr7QqaIlSe0VmVnKhicmJnJycrKU\nbUtSr4qI/Zk50e73dQayJMkwkCQZBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwD\nSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kScE7ZBai79kzNsGvfNM8fm+PS0RG2b1rP\nlo3jZZclqWSGwQDZMzXDzvsPMHf8JAAzx+bYef8BAANBGnB2Ew2QXfumTwVB1dzxk+zaN11SRZKW\nC8NggDx/bG5J7ZIGh2EwQC4dHVlSu6TBYRgMkO2b1jMyPHRa28jwENs3rS+pIknLhQPIA6Q6SOzZ\nRJLmayoMImIz8HFgCPiDzLyjwXo/AXwe+FeZOdm2KtU2WzaO++Uv6QyLdhNFxBBwJ3AtsAHYFhEb\n6qz3GuAXgUfaXaQkqbOaGTO4EjiUmc9k5qvAPcANddb7b8BHgX9qY32SpC5oJgzGgcM1j48UbadE\nxBXA6sz80kJvFBE3R8RkREzOzs4uuVhJUmec9dlEEbEC+Bjw/sXWzczdmTmRmRNjY2Nnu2lJUps0\nEwYzwOqax6uKtqrXAD8I/GVEPAtcBeyNiIl2FSlJ6qxmziZ6FFgXEWuphMBW4GeqT2bmy8DK6uOI\n+EvgVz2bqP94kTupfy0aBpl5IiJuAfZRObX0rsw8GBG3AZOZubfTRap8jS5yN/ncizz01VkDQupx\nkZmlbHhiYiInJz146BVX3/EgM3WuYRRA7SdoZHiI22+83ECQOiQi9mdm27vhvRyFmtLoYnbz/5Tw\nKqhSb/JyFGrKpaMjdY8M6qkXHI3GGxyHkJYHw0BN2b5p/WljBnBmF1HV/KugLjTe8Cf7Z7zZjrQM\n2E2kpmzZOM7tN17O+OgIAYyPjvDuqy5r6iqojW6qc/cjh73ZjrRMeGSgptW7yN3E6y9etJun0XjD\nyQYnL3izHan7DAOdlWaugtpovGEoom4geLMdqfvsJlLHNbqpzra3rPZmO9Iy4ZGBOm6hm+o0080k\nqfOcdCZJPcRJZ5KkjjEMJEmOGfSrVmb2OhtYGlyGQR9qNOMXGs/sbeU1kvqH3UR9qNGM34Vm9rby\nGkn9wzDoQ41m8C40s7eV10jqH4ZBH2o0g3ehmb2tvEZS/zAM+lCjGb8Lzext5TWS+ocDyH1ooRm/\n7XyNpP7hDGRJ6iHOQJYkdYzdRMuME78klcEwWEac+CWpLHYTLSNO/JJUFsNgGXHil6SyGAbLSKMJ\nXisiWLvjS1x9x4PsmZrpclWSBoFhsIzUm/gFlRvHJ98ZQzAQJLWbYbCMbNk4zu03Xs746AhB5Ybx\n8zmGIKkTmjqbKCI2Ax8HhoA/yMw75j3/K8B/BE4As8DPZeZzba61Lyx26uiWjeOnHq/d8aW67+EY\ngqR2W/TIICKGgDuBa4ENwLaI2DBvtSlgIjPfBHwe+I12F9oPqqeOzhyba6rbx4vHSeqWZrqJrgQO\nZeYzmfkqcA9wQ+0KmflQZr5SPHwYWNXeMvvDUk8d9eJxkrqlmTAYBw7XPD5StDVyE/Dn9Z6IiJsj\nYjIiJmdnZ5uvsk8s9dTR+WMI46Mj3H7j5U5Ak9R2bZ2BHBHvASaAf1vv+czcDeyGyoXq2rntXnDp\n6Agzdb74F+r2qR1DkKROaebIYAZYXfN4VdF2moh4B/AB4PrM/Of2lNdftm9az/DQ6WcIDQ+F3T6S\nStdMGDwKrIuItRFxLrAV2Fu7QkRsBD5FJQiOtr/MPjLveOj4yeQjXzjo3AFJpVo0DDLzBHALsA94\nGrgvMw9GxG0RcX2x2i7gQuBzEfFYROxt8HYD7SNfOMjxb5/ZO/bSK8edTCapVE2NGWTmA8AD89pu\nrVl+R5vrWhaWcjnpxdbdMzXDS68cb7it6llFjg9IKoOXsG5gKZeTrrfuL9/7GL9072OMF8HQzKxh\nJ5NJKouXo2ig2TkBe6ZmeP99j5+xbrUzqBoi9c4ims/JZJLKYhg00MycgOoRwclF7iM9d/xk3esM\n1XIymaQyGQYNNHMpiHpHD42czDxjNnE1HpxMJqlsjhk0sH3T+tPGAeD0v973TM001fVTVTt2sNCA\ntPdAllQGw6CB6hfwh/ce5Nhc5Syg84crB1LV7qFmVUNksdnE3gNZUlkMg0X884lvn1quzgc4f3hF\nU91DAUv6636hQWvDQFInGQYLaPTl3EwQXHTBMFO3vnNJ2/MeyJLK4gDyAlr9Eh4eCj704z+w5Nd5\n/wJJZRnoMNgzNcPVdzzY8GbzrXwJj4+OsOtdP9RSt473L5BUloHtJmo0a3jyuRf59S2Xs2dqhmOv\nvHrG60aGhzh/eEXdS0uMj47w1zve1nJN1QDxbCJJ3TawYVBvPCCBzz78DQDuffQwx0+eOZnsJ354\nnInXX7zgaadnw/sXSCrDwHYTNRoPSODuR+oHAcBDX531DmSS+s7AHhk0uusYsODlJaoh4l/wkvrJ\nwITB/Jm9P/rGMT778Dfm32tmUZ7ZI6kfDUQY1Bss/qNibGAphld4i0pJ/amvw6B6NLCUawjVuuiC\n4VNnDY2ODPPh63/AriFJfalvw2D+0cBSne1popLUS/r2bKKlXF56vgC7gyQNlL4Ng1a7hgJ491WX\n2R0kaaD0RTdR7ZlCozX9/Es1MryC2298k0EgaeD0fBjMHxtoJQhWBPzMWy7j17dc3u7yJKkn9GwY\nnO2ZQlW//dNv9khA0sDryTA42zOFqt7j2IAkAT06gPyRLxw86yAA7BaSpELPhcGeqZmWB4hrXXTB\ncBuqkaT+0FPdRB/cc6Cly0jMtyJo6U5kktSvmjoyiIjNETEdEYciYked58+LiHuL5x+JiDXtLrRd\nQRDAx37KQWNJqrVoGETEEHAncC2wAdgWERvmrXYT8FJm/kvgt4CPtrvQdgRBlUEgSadr5sjgSuBQ\nZj6Tma8C9wA3zFvnBuB/FMufB94eEdG+MtvHS1BL0pmaCYNx4HDN4yNFW911MvME8DLwuvlvFBE3\nR8RkREzOzs62VnGTLhhe4c3lJalJXT2bKDN3Z+ZEZk6MjY11bDsB/Pcb3+StKSWpSc2cTTQDrK55\nvKpoq7fOkYg4B3gt8EJbKlyi+Rea88tfkhbXzJHBo8C6iFgbEecCW4G989bZC7yvWH4X8GDmAjcS\nbsGzd/y7hs+tKEYnxkdH+K2ffrOTySRpiRY9MsjMExFxC7APGALuysyDEXEbMJmZe4E/BD4TEYeA\nF6kERtstFAiSpNY1NeksMx8AHpjXdmvN8j8BP9ne0iRJ3dJzl6OQJLWfYSBJMgwkSYaBJAmINp8B\n2vyGI2aB51p8+UrgH9tYTi9x3wfPoO43uO/19v31mdn2WbulhcHZiIjJzJwou44yuO+Dt++Dut/g\nvndz3+0mkiQZBpKk3g2D3WUXUCL3ffAM6n6D+941PTlmIElqr149MpAktZFhIEnqvTCIiM0RMR0R\nhyJiR9n1LCQi7oqIoxHxZE3bxRHxlYj4evHvRUV7RMQniv16IiKuqHnN+4r1vx4R76tp/+GIOFC8\n5hPVW422so027/fqiHgoIp6KiIMR8YsDtO/nR8TfRsTjxb5/pGhfGxGPFNu/t7gcPBFxXvH4UPH8\nmpr32lm0T0fEppr2ur8DrWyjQ/8HQxExFRFfHKR9j4hni8/kYxExWbT1zmc+M3vmh8oltP8OeANw\nLvA4sKHsuhao998AVwBP1rT9BrCjWN4BfLRYvg74cyr357kKeKRovxh4pvj3omL5ouK5vy3WjeK1\n17ayjQ7s9yXAFcXya4CvARsGZN8DuLBYHgYeKbZ3H7C1aP8k8J+L5f8CfLJY3grcWyxvKD7f5wFr\ni8/90EK/A0vdRgc/978C/DHwxVbq6tV9B54FVs5r65nPfOlfmEv8z34rsK/m8U5gZ9l1LVLzGk4P\ng2ngkmL5EmC6WP4UsG3+esA24FM17Z8q2i4BvlrTfmq9pW6jC/8Hfwb82KDtO3AB8H+Bt1CZSXrO\n/M8xlfuEvLVYPqdYL+Z/tqvrNfodKF6zpG10aJ9XAX8BvA34Yit19fC+P8uZYdAzn/le6yYaBw7X\nPD5StPWS783MbxbL/w/43mK50b4t1H6kTnsr2+iY4rB8I5W/kAdi34tukseAo8BXqPw1eywzT9TZ\n9qm6iudfBl63QL2N2l/XwjY64beBXwO+XTxupa5e3fcEvhwR+yPi5qKtZz7zTd3cRp2RmRkRHT23\ntxvbaCQiLgT+BPilzPxW0cXZtbrK2vfMPAm8OSJGgT8F3tjtGsoQEf8eOJqZ+yPimrLrKcGPZOZM\nRHwP8JWI+Grtk8v9M99rRwYzwOqax6uKtl7yDxFxCUDx79GivdG+LdS+qk57K9tou4gYphIEn83M\n+1usqyf3vSozjwEPUem2GI2I6h9ftds+VVfx/GuBFxaot1H7Cy1so92uBq6PiGeBe6h0FX28hbp6\ncd/JzJni36NU/gi4kh76zPdaGDwKrCvOHDiXyoDQ3pJrWqq9QPUMgfdR6U+vtr+3OAPgKuDl4tBv\nH/DOiLioOEvgnVT6Q78JfCsirirOKnjvvPdayjbaqqjnD4GnM/NjNU8Nwr6PFUcERMQIlbGSp6mE\nwrsa1FWt913Ag1np4N0LbC3OhlkLrKMygFj3d6B4zVK30VaZuTMzV2XmmqKuBzPz3S3U1XP7HhHf\nFRGvqS5T+aw+SS995jsxkNLJHyoj5F+j0g/7gbLrWaTWu4FvAsep9NfdRKW/8i+ArwP/G7i4WDeA\nO4v9OgBM1LzPzwGHip+frWmfKD5wfwf8Dt+ZUb7kbbR5v3+ESv/pE8Bjxc91A7LvbwKmin1/Eri1\naH8DlS+0Q8DngPOK9vOLx4eK599Q814fKOqdpjhzZKHfgVa20cHP/jV852yivt/3YvuPFz8Hq7X1\n0mfey1FIknqum0iS1AGGgSTJMJAkGQaSJAwDSRKGgSQJw0CSBPx/UNx7mqF8ZR0AAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4318b9510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vl1 = 2013\n",
    "vl2 = 2\n",
    "vl3 = 4\n",
    "par1 = 'year'\n",
    "par2 = 'shift'\n",
    "par3 = 'week'\n",
    "indexes = [i for i in range(len(train[par1])) if train[par1][i] == vl1 and\n",
    "           train[par2][i] == vl2 and\n",
    "           train[par3][i] == vl3\n",
    "          ]\n",
    "summ = [i.sum() for i in train[[\"f{}\".format(i) for i in range(1, 31)]].values[indexes]]\n",
    "pl.figure()\n",
    "pl.scatter(train[\"y\"].values[indexes], summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 0.09496255,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ]],\n",
       "\n",
       "       [[ 0.02729279,  0.02877978,  2.        ],\n",
       "        [ 0.01920622,  0.01897034,  0.02008428],\n",
       "        [ 0.04389617,  0.04313332,  0.04252565],\n",
       "        [ 0.04404178,  0.04357766,  0.04280375],\n",
       "        [ 0.05497197,  0.05306484,  0.05202828],\n",
       "        [ 0.03314322,  0.03319926,  0.03232362],\n",
       "        [ 0.03636816,  0.03575161,  0.03563468],\n",
       "        [ 0.04193399,  0.04153051,  0.04090014],\n",
       "        [ 0.06134198,  0.06028018,  0.05895115],\n",
       "        [ 0.03321884,  0.03278537,  0.03236254],\n",
       "        [ 0.03828457,  0.0379558 ,  0.03718776],\n",
       "        [ 0.04260191,  0.04191082,  0.04155099],\n",
       "        [ 0.05044471,  0.04980483,  0.04843602],\n",
       "        [ 0.0492685 ,  0.04810021,  0.04742032],\n",
       "        [ 0.04019248,  0.04027904,  0.03845114],\n",
       "        [ 0.04636179,  0.04645947,  0.04689996],\n",
       "        [ 0.05992048,  0.05937928,  0.05879649],\n",
       "        [ 0.05836914,  0.0575463 ,  0.05697166],\n",
       "        [ 0.04290388,  0.04313687,  0.04215175],\n",
       "        [ 0.05673838,  0.05638103,  0.05657613],\n",
       "        [ 0.05630332,  0.05634679,  0.0564687 ],\n",
       "        [ 0.08231098,  0.08207629,  0.08167459],\n",
       "        [ 0.04912859,  0.04910481,  0.04909334],\n",
       "        [ 0.04106512,  0.04125327,  0.04099992],\n",
       "        [ 0.06529623,  0.06483026,  0.06550818],\n",
       "        [ 0.06317336,  0.06359423,  0.0631362 ],\n",
       "        [ 0.05582741,  0.05558006,  0.05563546],\n",
       "        [ 0.05970303,  0.05997399,  0.05908847],\n",
       "        [ 0.06143491,  0.06106498,  0.06124483],\n",
       "        [ 0.06141441,  0.06153005,  0.06148135],\n",
       "        [ 0.06260127,  0.06050559,  0.06012076],\n",
       "        [ 0.05614467,  0.05713235,  0.05521445],\n",
       "        [ 0.05960921,  0.06082606,  0.06198166],\n",
       "        [ 0.06079829,  0.06117602,  0.06226401],\n",
       "        [ 0.06433212,  0.06468631,  0.06469972],\n",
       "        [ 0.04661965,  0.04675336,  0.04698027],\n",
       "        [ 0.04995499,  0.05031967,  0.05054423],\n",
       "        [ 0.05216614,  0.05248544,  0.05299895],\n",
       "        [ 0.054045  ,  0.05437991,  0.05455609],\n",
       "        [ 0.05028894,  0.0500952 ,  0.05043146],\n",
       "        [ 0.04564263,  0.0458715 ,  0.0455272 ],\n",
       "        [ 0.04967561,  0.05004889,  0.05031415],\n",
       "        [ 0.04633938,  0.04643245,  0.04677848],\n",
       "        [ 0.06667987,  0.06610783,  0.06639831],\n",
       "        [ 0.02629661,  0.02637712,  0.0261605 ],\n",
       "        [ 0.04684655,  0.04648883,  0.04718535],\n",
       "        [ 0.04800836,  0.04817509,  0.04780723],\n",
       "        [ 0.06055182,  0.06002194,  0.06032685],\n",
       "        [ 0.03597693,  0.03619652,  0.03592894],\n",
       "        [ 0.04478346,  0.04460343,  0.04485013],\n",
       "        [ 0.05757803,  0.05718752,  0.05693546],\n",
       "        [ 0.07706113,  0.07700861,  0.07697408],\n",
       "        [ 2.        ,  2.        ,  2.        ]],\n",
       "\n",
       "       [[ 0.04819282,  0.04789891,  0.0477759 ],\n",
       "        [ 0.01704234,  0.01702102,  0.01685575],\n",
       "        [ 0.03787581,  0.03739245,  0.03698387],\n",
       "        [ 0.0420184 ,  0.04106228,  0.04043864],\n",
       "        [ 0.05219397,  0.05112981,  0.0502043 ],\n",
       "        [ 0.03545257,  0.03553907,  0.0352963 ],\n",
       "        [ 0.04056062,  0.04005305,  0.03993139],\n",
       "        [ 0.04876248,  0.04802499,  0.0475393 ],\n",
       "        [ 0.04836187,  0.04748079,  0.04686741],\n",
       "        [ 0.0434643 ,  0.04299067,  0.04241187],\n",
       "        [ 0.03730309,  0.03695571,  0.03651035],\n",
       "        [ 0.04782196,  0.04722119,  0.04670462],\n",
       "        [ 0.04958267,  0.04895762,  0.04810574],\n",
       "        [ 0.05000074,  0.04900979,  0.04808941],\n",
       "        [ 0.04699075,  0.04686597,  0.04613424],\n",
       "        [ 0.05080305,  0.05026543,  0.05005508],\n",
       "        [ 0.05549071,  0.05501263,  0.05455825],\n",
       "        [ 0.06696641,  0.06704653,  0.06667919],\n",
       "        [ 0.05261322,  0.05264353,  0.05254845],\n",
       "        [ 0.05540423,  0.05525409,  0.05553656],\n",
       "        [ 0.05710979,  0.05688802,  0.05699849],\n",
       "        [ 0.06426107,  0.06411706,  0.06368507],\n",
       "        [ 0.05963277,  0.05872856,  0.05848429],\n",
       "        [ 0.05162075,  0.05244651,  0.05219531],\n",
       "        [ 0.0612547 ,  0.06103302,  0.06188163],\n",
       "        [ 0.0633334 ,  0.06348612,  0.06298456],\n",
       "        [ 0.05803631,  0.05752432,  0.05745199],\n",
       "        [ 0.05716989,  0.05778978,  0.0566965 ],\n",
       "        [ 0.05510853,  0.0551553 ,  0.05538936],\n",
       "        [ 0.05712136,  0.0567878 ,  0.05669007],\n",
       "        [ 0.06888585,  0.06710524,  0.06670406],\n",
       "        [ 0.04991682,  0.04995472,  0.04870612],\n",
       "        [ 0.05636765,  0.05741516,  0.05786646],\n",
       "        [ 0.05688632,  0.05740809,  0.05799384],\n",
       "        [ 0.0620477 ,  0.06220762,  0.06268945],\n",
       "        [ 0.04908479,  0.04912487,  0.04964219],\n",
       "        [ 0.04918692,  0.04959112,  0.04985397],\n",
       "        [ 0.05738533,  0.05843872,  0.05845618],\n",
       "        [ 0.05333076,  0.05373082,  0.05391012],\n",
       "        [ 0.05515885,  0.05537475,  0.05581972],\n",
       "        [ 0.04495826,  0.04520532,  0.04519606],\n",
       "        [ 0.04531699,  0.04567939,  0.04608717],\n",
       "        [ 0.04512861,  0.04501335,  0.04509515],\n",
       "        [ 0.05937979,  0.05911299,  0.05911808],\n",
       "        [ 0.02980298,  0.03001386,  0.03002772],\n",
       "        [ 0.0416643 ,  0.04125449,  0.0415165 ],\n",
       "        [ 0.04209204,  0.04175349,  0.04125547],\n",
       "        [ 0.04900785,  0.04873916,  0.04873643],\n",
       "        [ 0.04343759,  0.04292514,  0.04242679],\n",
       "        [ 0.04071455,  0.04043028,  0.03995331],\n",
       "        [ 0.04501788,  0.04443395,  0.04431006],\n",
       "        [ 0.06387543,  0.06346832,  0.06283772],\n",
       "        [ 0.06179641,  0.06164847,  0.06109724]],\n",
       "\n",
       "       [[ 0.01089516,  0.01085208,  0.01091223],\n",
       "        [ 0.03843993,  0.03769622,  0.03794805],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ],\n",
       "        [ 2.        ,  2.        ,  2.        ]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prediction(X):\n",
    "    answer = []\n",
    "    for i in range(len(X['year'])):\n",
    "        year, week, shift = X[['year', 'week', 'shift']].values[i]\n",
    "        answer.append(koefs[year - 2012, week - 1, shift - 1]*X[main_features].values[i].sum())\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = prediction(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.2951424264\n"
     ]
    }
   ],
   "source": [
    "result = 0\n",
    "for ans, right_ans in zip(pred, np.array(train['y'])):\n",
    "    result += abs(ans - right_ans) / (abs(right_ans) + abs(ans))\n",
    "    \n",
    "print result * 200 / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.tsv\")\n",
    "sample_submission = pd.read_csv(\"sample_submission.tsv\")\n",
    "sample_submission['y'] = prediction(test)\n",
    "sample_submission.to_csv(\"submission2.tsv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 31):\n",
    "    for j_1, j_2 in zip(train['f{}'.format(i)], train['f{}'.format(i + 30)]):\n",
    "        if j_1 != j_2:\n",
    "            print(\"different!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef print_params(params_arr, names_arr, params):\\n    for i, p, name in zip(range(len(params)), params, names_arr):\\n        if len(params_arr[i]) > 1:\\n            if isinstance(p, int):\\n                print \"{} = {}\".format(name, p), end=\"  \"\\n            else:\\n                print(\"{} = {:.4f}\".format(name, p), end=\"  \")  \\n\\ndef get_best_params(params_arr, names_arr, train, target, features):\\n    maximum = 200\\n    best_params = []\\n    for params in itertools.product(*params_arr):\\n        d = {}\\n        for i, name in enumerate(names_arr):\\n            d[name] = params[i]\\n        print_params(params_arr, names_arr, params)\\n        result = test_regressor(XGBRegressor(**d), train, target, features)\\n        print(\"SMPAE = {:.4f}\".format(result))\\n        if (result < maximum):\\n            maximum = result\\n            best_params = params\\n    print(\"best parametrs:\")\\n    print_params(params_arr, names_arr, best_params)\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SMAPE(regressor, X, y):\n",
    "    prediction = regressor.predict(X)\n",
    "    result = 0\n",
    "    for ans, right_ans in zip(prediction, np.array(y)):\n",
    "        result += abs(ans - right_ans) / (abs(right_ans) + abs(ans))\n",
    "    return result * 200 / len(y)\n",
    "    \n",
    "def test_regressor(regressor, data, target, features):\n",
    "    tscv = model_selection.TimeSeriesSplit(n_splits=5)\n",
    "    score = []\n",
    "    for train_index, test_index in tscv.split(data):\n",
    "        X_train, X_test = data[features].values[train_index], data[features].values[test_index]\n",
    "        y_train, y_test = data[target].values[train_index], data[target].values[test_index]\n",
    "        regressor.fit(X_train, y_train, eval_metric='mae')\n",
    "        score.append(SMAPE(regressor, X_test, y_test))\n",
    "\n",
    "    return np.array(score).mean()\n",
    "\"\"\"\n",
    "def print_params(params_arr, names_arr, params):\n",
    "    for i, p, name in zip(range(len(params)), params, names_arr):\n",
    "        if len(params_arr[i]) > 1:\n",
    "            if isinstance(p, int):\n",
    "                print \"{} = {}\".format(name, p), end=\"  \"\n",
    "            else:\n",
    "                print(\"{} = {:.4f}\".format(name, p), end=\"  \")  \n",
    "\n",
    "def get_best_params(params_arr, names_arr, train, target, features):\n",
    "    maximum = 200\n",
    "    best_params = []\n",
    "    for params in itertools.product(*params_arr):\n",
    "        d = {}\n",
    "        for i, name in enumerate(names_arr):\n",
    "            d[name] = params[i]\n",
    "        print_params(params_arr, names_arr, params)\n",
    "        result = test_regressor(XGBRegressor(**d), train, target, features)\n",
    "        print(\"SMPAE = {:.4f}\".format(result))\n",
    "        if (result < maximum):\n",
    "            maximum = result\n",
    "            best_params = params\n",
    "    print(\"best parametrs:\")\n",
    "    print_params(params_arr, names_arr, best_params)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target = ['y']\n",
    "features = [x for x in train.columns if x not in [target, 'item_id', 'year', 'weak', 'shift']]\n",
    "param_names = ['learning_rate',\n",
    "               'n_estimators',\n",
    "               'max_depth',\n",
    "               'min_child_weight',\n",
    "               'gamma',\n",
    "               'subsample',\n",
    "               'colsample_bytree',\n",
    "               'nthread',\n",
    "               'seed',\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params_1 = [[0.1], range(100, 110, 2), [5], [1], [0], [0.8], [0.8], [4], [27]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100  SMPAE = 18.9412\n",
      "n_estimators = 102  SMPAE = 18.8868\n",
      "n_estimators = 104  SMPAE = 18.9182\n",
      "n_estimators = 106  SMPAE = 18.9693\n",
      "n_estimators = 108  SMPAE = 19.0434\n",
      "best parametrs:\n",
      "n_estimators = 102  "
     ]
    }
   ],
   "source": [
    "get_best_params(params_1, param_names, train, target, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 3  min_child_weight = 1  SMPAE = 28.2304\n",
      "max_depth = 3  min_child_weight = 2  SMPAE = 29.1355\n",
      "max_depth = 3  min_child_weight = 3  SMPAE = 30.1970\n",
      "max_depth = 3  min_child_weight = 4  SMPAE = 30.4533\n",
      "max_depth = 3  min_child_weight = 5  SMPAE = 30.2757\n",
      "max_depth = 4  min_child_weight = 1  SMPAE = 22.3518\n",
      "max_depth = 4  min_child_weight = 2  SMPAE = 23.2648\n",
      "max_depth = 4  min_child_weight = 3  SMPAE = 24.9585\n",
      "max_depth = 4  min_child_weight = 4  SMPAE = 25.4751\n",
      "max_depth = 4  min_child_weight = 5  SMPAE = 25.6518\n",
      "max_depth = 5  min_child_weight = 1  SMPAE = 18.8868\n",
      "max_depth = 5  min_child_weight = 2  SMPAE = 19.1192\n",
      "max_depth = 5  min_child_weight = 3  SMPAE = 19.7433\n",
      "max_depth = 5  min_child_weight = 4  SMPAE = 19.6694\n",
      "max_depth = 5  min_child_weight = 5  SMPAE = 25.0435\n",
      "max_depth = 6  min_child_weight = 1  SMPAE = 18.5929\n",
      "max_depth = 6  min_child_weight = 2  SMPAE = 20.7352\n",
      "max_depth = 6  min_child_weight = 3  SMPAE = 21.5878\n",
      "max_depth = 6  min_child_weight = 4  SMPAE = 20.7536\n",
      "max_depth = 6  min_child_weight = 5  SMPAE = 16.3153\n",
      "max_depth = 7  min_child_weight = 1  SMPAE = 12.1084\n",
      "max_depth = 7  min_child_weight = 2  SMPAE = 13.8099\n",
      "max_depth = 7  min_child_weight = 3  SMPAE = 13.1150\n",
      "max_depth = 7  min_child_weight = 4  SMPAE = 12.9891\n",
      "max_depth = 7  min_child_weight = 5  SMPAE = 13.1018\n",
      "max_depth = 8  min_child_weight = 1  SMPAE = 10.1525\n",
      "max_depth = 8  min_child_weight = 2  SMPAE = 11.1155\n",
      "max_depth = 8  min_child_weight = 3  SMPAE = 11.3661\n",
      "max_depth = 8  min_child_weight = 4  SMPAE = 11.1056\n",
      "max_depth = 8  min_child_weight = 5  SMPAE = 11.3483\n",
      "max_depth = 9  min_child_weight = 1  SMPAE = 9.2712\n",
      "max_depth = 9  min_child_weight = 2  SMPAE = 10.0581\n",
      "max_depth = 9  min_child_weight = 3  SMPAE = 10.1523\n",
      "max_depth = 9  min_child_weight = 4  SMPAE = 10.0071\n",
      "max_depth = 9  min_child_weight = 5  SMPAE = 10.3159\n",
      "best parametrs:\n",
      "max_depth = 9  min_child_weight = 1  "
     ]
    }
   ],
   "source": [
    "params_2 = [[0.1], [102], range(3, 10), range(1, 6), [0], [0.8], [0.8], [4], [27]]\n",
    "get_best_params(params_2, param_names, train, target, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 0.0000  SMPAE = 9.2712\n",
      "gamma = 0.0214  SMPAE = 9.2712\n",
      "gamma = 0.0429  SMPAE = 9.2712\n",
      "gamma = 0.0643  SMPAE = 9.2712\n",
      "gamma = 0.0857  SMPAE = 9.2712\n",
      "gamma = 0.1071  SMPAE = 9.2712\n",
      "gamma = 0.1286  SMPAE = 9.2712\n",
      "gamma = 0.1500  SMPAE = 9.2712\n",
      "gamma = 0.1714  SMPAE = 9.2712\n",
      "gamma = 0.1929  SMPAE = 9.2712\n",
      "gamma = 0.2143  SMPAE = 9.2712\n",
      "gamma = 0.2357  SMPAE = 9.2712\n",
      "gamma = 0.2571  SMPAE = 9.2712\n",
      "gamma = 0.2786  SMPAE = 9.2712\n",
      "gamma = 0.3000  SMPAE = 9.2712\n",
      "best parametrs:\n",
      "gamma = 0.0000  "
     ]
    }
   ],
   "source": [
    "params_3 = [[0.1], [102], [9], [1], np.linspace(0, 0.3, 15), [0.8], [0.8], [4], [27]]\n",
    "get_best_params(params_3, param_names, train, target, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params_4 = [[0.1], [102], [9], [1], [0], np.linspace(0.6, 0.9, 5),\n",
    "            np.linspace(0.6, 0.9, 5), [4], [27]]\n",
    "get_best_params(params_4, param_names, train, target, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
